{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f9f912-cb3c-43e0-b8d4-ef9cc467fe4e",
   "metadata": {},
   "source": [
    "## GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c19b2b8-d891-4e13-ac18-1d1353e42bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /nfs/storage1/home/sammartj/.local/lib/python3.7/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /nfs/storage1/home/sammartj/.local/lib/python3.7/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /nfs/storage1/home/sammartj/.local/lib/python3.7/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /nfs/storage1/home/sammartj/.local/lib/python3.7/site-packages (from click->nltk) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /nfs/storage1/home/sammartj/.local/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413e7a35-2465-4a8e-b07a-79d4286771a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sammartj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2118e2e-14b8-4969-9dc0-321b7184d2ac",
   "metadata": {},
   "source": [
    "There are two files given to you, one a .txt file labelled as a 'PMC' followed by numbers, this is an ID for an article, \n",
    "and a .json file with an entity object that matches with the PMC id. Extract relational information from the .txt file \n",
    "that relates to ONLY the biomarkers listed for THAT article in the .json file. Not every entity in the .json file is necessarily a\n",
    "biomarker, so you need to decide which ones are biomarkers before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fbb3d6-958c-4c6a-b09e-972564ba9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "There is a file given to you labelled as a 'PMC' followed by numbers, this is an ID for an article. Extract any named biomarkers and\n",
    "their relational information from the .txt file as it pertains to Alzheimer's Disease. If no information containing biomarkers is described, \n",
    "you may return an emtpy .json.\n",
    "\n",
    "Use ONLY the following relationship types, and you may use more than one for each biomarker (but using only one is acceptable too):\n",
    "- DIAGNOSITC_OF\n",
    "- INDICATES_PRESENCE_OF\n",
    "- USED_TO_DIAGNOSE\n",
    "- RELATED_PROGNOSIS\n",
    "- ASSOCIATED_WITH_POOR_OUTCOME\n",
    "- ASSOCIATED_WITH_BETTER_OUTCOME\n",
    "- PREDICTS_SURVIVAL\n",
    "- PREDICTS_MORTALITY\n",
    "- RELATED_TREATMENT\n",
    "- GUIDES_TREATMENT_SELECTION\n",
    "- INDICATES_TREATMENT_ELIGIBILITY\n",
    "- IS_TARGET_OF_TREATMENT\n",
    "- IS_BASIS_FOR_THERAPY_CHOICE\n",
    "- RESPONDED_TO\n",
    "- MONITORED_DURING_TREATMENT\n",
    "- LEVELS_REFLECT_RESPONSE\n",
    "- BIOMARKER_DYNAMICS_UNDER_TREATMENT\n",
    "Return a list of JSON objects. For example:\n",
    "[\n",
    "    {\n",
    "        \"article\": \"PMC00000034.txt\",\n",
    "        \"subject\": \"APOE4\",\n",
    "        \"relationship\": \"USED_TO_DIAGNOSE\",\n",
    "        \"object\": \"Diagnose: Early-onset Alzheimer's Disease\"\n",
    "    }\n",
    "]\n",
    "\n",
    "- ONLY return sets of four and nothing else. None of 'article', 'subject', 'relationship' and 'object' can be empty, unless no biomarkers\n",
    "aren mentioned. Return the response in a JSON format. \n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a7477-ed11-401e-a9dc-a417346a61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading .txt articles:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[429] Rate limit hit. Waiting 61.0 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 60.1 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 120.7 seconds before retrying...\n",
      "No valid JSON found in PMC8615710.txt, chunk 4. Output:\n",
      "[ ]\n",
      "\n",
      "```json\n",
      " []  \n",
      "```\n",
      "[429] Rate limit hit. Waiting 60.7 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 60.7 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 120.6 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 240.4 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 60.9 seconds before retrying...\n",
      "No valid JSON found in PMC8615710.txt, chunk 8. Output:\n",
      "```json\n",
      "[ ]\n",
      "```\n",
      "[429] Rate limit hit. Waiting 60.7 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading .txt articles:   3%|â–Ž         | 1/30 [14:10<6:50:59, 850.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid JSON found in PMC8615710.txt, chunk 10. Output:\n",
      "```json\n",
      "[]\n",
      "```\n",
      "[429] Rate limit hit. Waiting 60.1 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 121.0 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 240.7 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 60.1 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 120.9 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 240.1 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 480.1 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 60.4 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 120.8 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 240.4 seconds before retrying...\n",
      "Secondary parse failed for PMC8638526.txt, chunk 6.\n",
      "[429] Rate limit hit. Waiting 60.6 seconds before retrying...\n",
      "[429] Rate limit hit. Waiting 120.5 seconds before retrying...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# Configs\n",
    "api_key = \"\" \n",
    "model = \"gemma2-9b-it\" # \"deepseek-r1-distill-llama-70b\"\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "\n",
    "txt_dir = \"../data/target\"\n",
    "ner_json_path = \"ner_results.json\"\n",
    "output_file = \"rel_ner_results.jsonl\"\n",
    "max_chunk_tokens = 1000 \n",
    "\n",
    "with open(ner_json_path, 'r', encoding='utf-8') as f:\n",
    "    ner_data = json.load(f)\n",
    "processed_articles = set()\n",
    "if os.path.exists(\"processed.txt\"):\n",
    "    with open(\"processed.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        processed_articles = set(line.strip() for line in f)\n",
    "\n",
    "# Prepare output\n",
    "for txt_file in tqdm(os.listdir(txt_dir), desc= \"Loading .txt articles\"):\n",
    "    # if not txt_file.endswith(\".txt\") or txt_file not in ner_data:\n",
    "    if not txt_file.endswith(\".txt\") or txt_file in processed_articles:\n",
    "        continue\n",
    "    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "\n",
    "        txt_path = os.path.join(txt_dir, txt_file)\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Sentence tokenize\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Chunk sentences into groups\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_len = 0\n",
    "\n",
    "        for sent in sentences:\n",
    "            token_count = len(sent.split())  \n",
    "            if current_len + token_count > max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    current_chunk = []\n",
    "                    current_len = 0\n",
    "            current_chunk.append(sent)\n",
    "            current_len += token_count\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        # Get known biomarkers\n",
    "        biomarkers = [entry[\"word\"] for entry in ner_data[txt_file]]\n",
    "\n",
    "        # Send each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            user_prompt = f\"\"\" This is a section from {txt_file} (chunk {i + 1} of {len(chunks)}): {chunk} \n",
    "            The following are the known biomarkers from the corresponding JSON: {json.dumps(biomarkers)}\n",
    "            Extract relationships as per instructions. \"\"\"\n",
    "\n",
    "\n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                MAX_RETRIES = 5\n",
    "                BASE_DELAY = 60  # seconds\n",
    "\n",
    "                # response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "                # response.raise_for_status()\n",
    "                def send_with_retries(payload, retries=MAX_RETRIES):\n",
    "                    for attempt in range(retries):\n",
    "                        try:\n",
    "                            response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "                            if response.status_code == 429:\n",
    "                                raise requests.exceptions.HTTPError(\"Rate limit hit (429)\", response=response)\n",
    "                            response.raise_for_status()\n",
    "                            return response.json()\n",
    "                        except requests.exceptions.HTTPError as e:\n",
    "                            if response.status_code == 429:\n",
    "                                wait_time = BASE_DELAY * (2 ** attempt) + uniform(0, 1)  # exponential backoff with jitter\n",
    "                                print(f\"[429] Rate limit hit. Waiting {wait_time:.1f} seconds before retrying...\")\n",
    "                                time.sleep(wait_time)\n",
    "                            else:\n",
    "                                print(f\"HTTP error on attempt {attempt + 1}: {e}\")\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error on attempt {attempt + 1}: {e}\")\n",
    "                            break\n",
    "                    return None\n",
    "                # result = response.json()\n",
    "                result = send_with_retries(payload)\n",
    "                if not result:\n",
    "                    print(f\"Skipping {txt_file}, chunk {i + 1} due to repeated errors.\")\n",
    "                    continue\n",
    "                output_text = result['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "                output_text = result['choices'][0]['message']['content']\n",
    "\n",
    "                # Save notes separately\n",
    "                with open(\"processed.txt\", \"a\", encoding=\"utf-8\") as notes_file:\n",
    "                    output_text = result['choices'][0]['message']['content']\n",
    "\n",
    "                    # Extract reasoning from <think>...</think>\n",
    "                    think_match = re.search(r\"<think>(.*?)</think>\", output_text, re.DOTALL)\n",
    "                    if think_match:\n",
    "                        notes_file.write(f\"### {txt_file} | Chunk {i + 1} ###\\n\")\n",
    "                        notes_file.write(think_match.group(1).strip() + \"\\n\\n\")\n",
    "\n",
    "                    # Everything after </think> (or full content if no <think>)\n",
    "                    post_think_match = re.split(r\"</think>\", output_text, maxsplit=1)\n",
    "                    post_think_text = post_think_match[1].strip() if len(post_think_match) > 1 else output_text.strip()\n",
    "\n",
    "                    # Try parsing the post-<think> content as JSON\n",
    "                    try:\n",
    "                        parsed_output = json.loads(post_think_text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Use regex to find JSON array if wrapped in text\n",
    "                        match = re.search(r\"\\[\\s*{.*?}\\s*\\]\", post_think_text, re.DOTALL)\n",
    "                        if match:\n",
    "                            try:\n",
    "                                parsed_output = json.loads(match.group(0))\n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Secondary parse failed for {txt_file}, chunk {i + 1}.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(f\"No valid JSON found in {txt_file}, chunk {i + 1}. Output:\\n{post_think_text}\")\n",
    "                            continue\n",
    "\n",
    "                    # Optional: skip empty results\n",
    "                    if not parsed_output:\n",
    "                        continue\n",
    "\n",
    "                    # Save valid entries to .jsonl\n",
    "                    for relation in parsed_output:\n",
    "                        if all(k in relation and relation[k] for k in ['article', 'subject', 'relationship', 'object']):\n",
    "                            relation[\"chunk\"] = i + 1\n",
    "                            relation[\"article\"] = txt_file\n",
    "                            outfile.write(json.dumps(relation) + '\\n')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error on {txt_file}, chunk {i + 1}: {e}\")\n",
    "                \n",
    "    with open(\"processed.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt_file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a937c2-2706-4f09-982d-2c8ef477cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('ner_results.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a455bcc-7222-4223-8f86-dbe6292afbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Step 1: Extract unique nodes\n",
    "nodes = set()\n",
    "for triple_list in all_triples:\n",
    "    for triplet in triple_list:\n",
    "        # Add \"subject\" if it exists\n",
    "        if \"subject\" in triplet:\n",
    "            nodes.add((triplet[\"subject\"], \"Entity\"))\n",
    "        # Add \"object\" if it exists\n",
    "        if \"object\" in triplet:\n",
    "            nodes.add((triplet[\"object\"], \"Entity\"))\n",
    "\n",
    "# Step 2: Write nodes.\n",
    "\n",
    "with open(\"nodes-r1.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    writer.writerows(nodes)\n",
    "\n",
    "# Step 3: Extract relationships\n",
    "edges = []\n",
    "for triple_list in all_triples:\n",
    "    for triplet in triple_list:\n",
    "        if \"subject\" in triplet and \"object\" in triplet and \"relationship\" in triplet:\n",
    "            edges.append((triplet[\"subject\"], triplet[\"object\"], triplet[\"relationship\"]))\n",
    "\n",
    "# Step 4: Write edges.csv\n",
    "with open(\"edges-r1.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"source\", \"target\", \"relationship\"])\n",
    "    writer.writerows(edges)\n",
    "\n",
    "print(\"Nodes and edges files generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19c9e4-c337-447b-8777-bb19bc6a21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo cp nodes.csv /var/lib/neo4j/import/\n",
    "# sudo cp edges.csv /var/lib/neo4j/import/\n",
    "# cypher-shell -u neo4j -p <your_password>\n",
    "# LOAD CSV WITH HEADERS FROM 'file:///nodes.csv' AS row\n",
    "# CREATE (:Entity {id: row.id, label: row.label});\n",
    "# LOAD CSV WITH HEADERS FROM 'file:///edges-r1.csv' AS row\n",
    "# MATCH (a:Entity {id: row.source}), (b:Entity {id: row.target})\n",
    "# CREATE (a)-[:RELATIONSHIP {type: row.relationship}]->(b);\n",
    "# MATCH (n)\n",
    "# OPTIONAL MATCH (n)-[r]-()\n",
    "# DELETE n,r\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'file:///nodes-r1.csv' AS row\n",
    "CREATE (:Entity {id: row.id, label: row.label});\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'file:///edges-r1.csv' AS row\n",
    "WITH row\n",
    "CALL {\n",
    "    // Match source and target nodes\n",
    "    MATCH (a:Entity {id: row.source})\n",
    "    MATCH (b:Entity {id: row.target})\n",
    "\n",
    "    // Create relationship with dynamic label\n",
    "    CREATE (a)-[r:`${row.relationship}`]->(b)\n",
    "}\n",
    "RETURN count(*);\n",
    "# ALL DATABASE\n",
    "# MATCH (n)-[r]->(m) RETURN n, r, m;\n",
    "# QUERY RELATIONSHIPS\n",
    "# MATCH (n)-[r]->(m) RETURN r LIMIT 10;\n",
    "# QUERY PATIENT ALL DIAGNOSES\n",
    "# MATCH (patient:Entity {id: \"Patient ID: 10000980\"})-[:RELATIONSHIP{type: \"has\"}]->(diagnosis:Entity)\n",
    "# RETURN diagnosis.id AS diagnosis_id;\n",
    "# QUERY ALL THE MEDICATION\n",
    "# MATCH (patient:Entity {id: \"Patient ID: 10000980\"})-[:RELATIONSHIP{type: \"receives\"}]->(medication:Entity)\n",
    "# RETURN medication.id AS medication_id\n",
    "# QUERY ALL THE MEDICATION IN THE DB\n",
    "# MATCH (patient:Entity)-[:RELATIONSHIP {type: \"receives\"}]->(medication:Entity)\n",
    "# RETURN medication.id AS medication_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3fb03-6906-470a-b641-82f8d6f9329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_relationship(relationship):\n",
    "    # Remove spaces and special characters, replace with underscores\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9]', '_', relationship)\n",
    "    # Convert to uppercase\n",
    "    return cleaned.upper()\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "\n",
    "def normalize_relationship(relationship):\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9]', '_', relationship)\n",
    "    return cleaned.upper()\n",
    "\n",
    "URI = \"bolt://localhost:7688\"\n",
    "AUTH = (\"neo4j\", \"12345678\")\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "\n",
    "    def create_relationship(source_id, target_id, relationship_type):\n",
    "        with driver.session() as session:\n",
    "            query = \"\"\"\n",
    "                MATCH (a:Entity {id: $source_id})\n",
    "                MATCH (b:Entity {id: $target_id})\n",
    "                CREATE (a)-[r:%s]->(b)\n",
    "            \"\"\" % relationship_type\n",
    "            parameters = {\n",
    "                \"source_id\": source_id,\n",
    "                \"target_id\": target_id\n",
    "            }\n",
    "            session.run(query, parameters)\n",
    "\n",
    "    # Read edges file and process each row\n",
    "    with open(\"edges-r1.csv\", mode='r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            source_id = row['source']\n",
    "            target_id = row['target']\n",
    "            relationship_type = normalize_relationship(row['relationship'])\n",
    "            create_relationship(source_id, target_id, relationship_type)\n",
    "\n",
    "    print(\"All relationships created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
